job "prometheus" {
    datacenters = ["dc1"]
    type        = "service"

    group "prometheus" {
  	    network {
  		    port "prometheus_ui" {
    	        to = 9090
      	         static = 9090
		    }
	    }
        service {
	        name = "prometheus"
            port = "prometheus_ui"
            tags = [
      	        "metrics"
            ]
        }
	    task "prometheus" {
            template {
                change_mode = "noop"
                destination = "local/prometheus.yml"

                data = <<EOH
---
global:
  scrape_interval:     5s
  evaluation_interval: 5s

alerting:
  alertmanagers:
  # List of Consul service discovery configurations.
  - consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['alertmanager']

rule_files:
  - "/etc/alertmanager/infra.rules"

scrape_configs:

  - job_name: 'consul'

    consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['consul-exporter']

    relabel_configs:
    - source_labels: [__meta_consul_service]
      target_label: job

    scrape_interval: 5s
    metrics_path: /metrics
    params:
      format: ['prometheus']

  - job_name: 'nomad_metrics'

    consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['nomad', 'nomad-client']

    relabel_configs:
    - source_labels: [__meta_consul_tags]
      separator: ;
      regex: (.*)http(.*)
      replacement: $1
      action: keep
    - source_labels: [__meta_consul_address]
      separator: ;
      regex: (.*)
      target_label: __meta_consul_service_address
      replacement: $1
      action: replace

    scrape_interval: 5s
    metrics_path: /v1/metrics
    params:
      format: ['prometheus']

  - job_name: 'nodes'

    consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['node-exporter']

    relabel_configs:
    - source_labels: [__meta_consul_service]
      target_label: job
    
    scrape_interval: 5s
    metrics_path: /metrics
    params:
      format: ['prometheus']

  - job_name: 'prometheus'

    consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['prometheus']

    relabel_configs:
    - source_labels: [__meta_consul_service]
      target_label: job
    
    scrape_interval: 5s
    metrics_path: /metrics
    params:
      format: ['prometheus']

  - job_name: 'alertmanager'

    consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['alertmanager']

    relabel_configs:
    - source_labels: [__meta_consul_service]
      target_label: job

    scrape_interval: 5s
    metrics_path: /metrics
    params:
      format: ['prometheus']

  - job_name: 'webserver'

    consul_sd_configs:
    - server: '10.0.0.10:8500'
      services: ['apache-exporter']

    relabel_configs:
    - source_labels: [__meta_consul_service]
      target_label: job
    
    scrape_interval: 5s
    metrics_path: /metrics
    params:
      format: ['prometheus']
EOH
      }
            template {
                change_mode = "noop"
                destination = "local/infra.rules"
                data = <<EOH
groups:
  - name: Prometheus rules
    rules:
      - alert: PrometheusJobMissing
        expr: absent(up{job="prometheus"})
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Prometheus job missing 
          description: A Prometheus job has disappeared
      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus target missing 
          description: A Prometheus target has disappeared. An exporter might be crashed.
      - alert: PrometheusAllTargetsMissing
        expr: count by (job) (up) == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus all targets missing 
          description: A Prometheus job does not have living target anymore.
      - alert: PrometheusAlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Prometheus AlertManager configuration reload failure 
          description: AlertManager configuration reload error
      - alert: PrometheusNotConnectedToAlertmanager
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus not connected to alertmanager 
          description: Prometheus cannot connect the alertmanager
      - alert: PrometheusTargetEmpty
        expr: prometheus_sd_discovered_targets == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus target empty 
          description: Prometheus has no target in service discovery
      # Alert when an instance is down for 2 minutes
      - alert: instance_down_2m
        expr: absent(up{job="node-exporter"}) == 1
        for: 2m 
        labels: 
          severity: Critical
        annotations: 
          description: Instance prometheus DOWN for 2 minutes
          summary: Instance DOWN
      - alert: PrometheusAlertmanagerE2eDeadManSwitch
        expr: vector(1)
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Prometheus AlertManager E2E dead man switch
          description: Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager
  - name: Node-exporter rules
    rules:
      - alert: HostOutOfMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host out of memory 
          description: Node memory is filling up (< 10% left)
      - alert: HostOutOfDiskSpace
        expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host out of disk space 
          description: Disk is almost full (< 10% left)
      - alert: HostOutOfInodes
        expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: Host out of inodes 
          description: Disk is almost running out of available inodes (< 10% left)
      - alert: HostHighCpuLoad
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Host high CPU load 
          description: CPU load is > 80% 
  - name: Apache rules
    rules:  
      - alert: ApacheDown
        expr: apache_up == 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Apache down
          description: Apache down
      - alert: ApacheRestart
        expr: apache_uptime_seconds_total / 60 < 1
        for: 0m
        labels:
          severity: warning
        annotations:
          summary: Apache restart 
          description: Apache has just been restarted.
  - name: Consul rules
    rules:
      - alert: ConsulServiceHealthcheckFailed
        expr: consul_catalog_service_node_healthy == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: Consul service healthcheck failed 
          description: Service:  Healthcheck: 
      - alert: ConsulAgentUnhealthy
        expr: consul_health_node_status{status="critical"} == 1
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: Consul agent unhealthy 
          description: A Consul agent is down
EOH
        }
            driver = "docker"
            config {
      	        image = "prom/prometheus:latest"
                ports = ["prometheus_ui"]
                logging {
                    type = "journald"
                    config {
          	            tag = "PROMETHEUS"
                    }
                }   
                volumes = [
                    "local/prometheus.yml:/etc/prometheus/prometheus.yml",
                    "local/infra.rules:/etc/alertmanager/infra.rules",       
                ]
            }
            resources {
      	        memory = 100
            }
  	    } 
    }
}